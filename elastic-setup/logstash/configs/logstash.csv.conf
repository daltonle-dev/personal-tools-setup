input {
    file {
        path => "path_to_csv_file/file_name.csv"
        # instruct logstash read from beginning, this setting only applies to file that are read for the first time.
        # By default, program would prefer to read from the end.
        # Since it processes the file that would have data added to it periodically.
        start_position => "beginning"
        # sincedb_path points to the db file that keeps track of the last parsed in the input file.
        # Set to "/dev/null" will process entire the file each time from beginning 
        # rather than continue from where it left off.
        sincedb_path => "/dev/null"
    }
}

filter {
    csv {
        separator => ","
        skip_header => "true"
        columns => ["id","timestamp","paymentType", "name", "gender", "ip_address", "purpose", "country", "age"]
    }

    mutate {
        convert => {
            "age" => "integer"
        }
        remove_field => ["message", "@timestamp", "path", "host", "@version"]
    }
}

output {
    stdout {
    }

    elasticsearch {
        index => "logstash-%{+YYYY.MM.dd}"
        hosts => "${ELASTIC_HOSTS}"
        user => "${ELASTIC_USER}"
        password => "${ELASTIC_PASSWORD}"
        cacert => "certs/ca/ca.crt"
    }
}
